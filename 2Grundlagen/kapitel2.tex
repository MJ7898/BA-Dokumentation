%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Descr:       Vorlage fÃ¼r Berichte der DHBW-Karlsruhe, Ein Kapitel
%% Author:      Prof. Dr. JÃ¼rgen Vollmer, vollmer@dhbw-karlsruhe.de
%% $Id: kapitel2.tex,v 1.5 2017/10/06 14:02:51 vollmer Exp $
%%  -*- coding: utf-8 -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Grundlagen}
\label{chap:Grundlagen}
In diesem Kapitel werden die fÃ¼r diese Bachelorarbeit notwendigen Grundlagen geschaffen, um ein fundiertes Wissen und VerstÃ¤ndnis 
Ã¼ber verwendete Technologien zu schaffen. Auf alle diese Informationen und Voraussetzungen wird im Folgenden eingegangen, um nachfolgende 
Konzeption und Umsetzung besser zu verstehen.

\section{Augmented Reality}
\label{chap:Augmented Reality}
Eine der wichtigsten Grundlagen dieser Arbeit ist das VerstÃ¤ndnis des Begriffs der Augmented Reality.
\\ 
\acl{AR}, deutsch erweiterte RealitÃ¤t, ist eine durch den Computer gestÃ¼tzte Erweiterung der RealitÃ¤t, bzw. der menschlichen 
Wahrnehmung. Es ermÃ¶glicht dem Nutzer die reale Welt mit Ãœberlagerung oder Zusammensetzung virtueller Objekte und visueller Informationen
zu sehen. Mittels einer Art Overlay werden diese Objekte und Informationen Ã¼ber die reale Welt gelegt und dem Nutzer zur VerfÃ¼gung gestellt. 
Allgemein soll damit dem Nutzer ein weit gefÃ¤cherter Ãœberblick verschafft werden und Hilfestellung leisten, aber den Nutzer in keinerlei 
Interaktion mit der Umgebung einschrÃ¤nken. Die Definition, welche sich in der Wissenschaft weitestgehend durchgesetzt und etabliert hat ist 
die Definition nach Azuma aus dem Jahre 1997.
\begin{quote}
    â€Augmented Reality (AR) is a variation of Virtual Environments (VE), or Virtual Reality as it is more commonly called. VE 
    technologies completely immerse a user inside a synthetic environment. While immersed, the user cannot see the real world around him. 
    In contrast, AR allows the user to see the real world, with virtual objects superimposed upon or composited with the real world. 
    Therefore, AR supplements reality, rather than completely replacing it.â€œ \cite{azuma.1997a}
\end{quote}
Ein Augmented Reality System verfÃ¼gt nach \cite{azuma.1997a} Ã¼ber folgende drei charakteristische Merkmale: 
\begin{enumerate}
    \item Es kombiniert RealitÃ¤t und VirtualitÃ¤t.
    \item Es ist interaktiv in Echtzeit.
    \item Die virtuellen Inhalte sind im 3D registriert.
\end{enumerate}
Das erste genannte Merkmal kombiniert die reale Welt mit dem oben genannten Overlay, der Ãœberlagerung der RealitÃ¤t um kÃ¼nstliche virtuelle 
Objekte und visuelle Informationen. Dies bedeutet, der Nutzer nimmt die reale Umgebung gleichzeitig mit den darin liegenden virtuellen 
Objekten als ein Ganzes wahr. Daraus resultiert die Interaktion von virtuellen Objekten und Informationen mit der realen Welt in Echtzeit, 
damit sie als Teil der RealitÃ¤t registriert werden kÃ¶nnen. Das dritte Merkmal umfasst die Darstellung von Objekten als scheinbar reales 
Objekt. Mit dem letzt genannten Merkmal wird das Ziel verfolgt die projizierten, bzw. nicht realen Teile tÃ¤uschend echt in die Umgebung zu 
integrieren.
\\ 
\linebreak  
%DarÃ¼ber hinaus sind die virtuellen Inhalte in 3D (d. h. geometrisch) registriert. Dies bedeutet nichts anderes, als dass in einer 
%AR-Umgebung ein virtuelles Objekt scheinbar einen festen Platz in RealitÃ¤t hat und diesen, sofern es nicht durch eine Benutzerinteraktion 
%verÃ¤ndert wird oder sich z. B. in Form einer Animation selbst verÃ¤ndert, auch beibehÃ¤lt. Mit anderen Worten: Es verhÃ¤lt sich aus Nutzersicht 
%genauso, wie ein reales Objekt, was sich an diesem Ort befinden wÃ¼rde
Eine etwas allgemein formuliertere Definition ist die nach \cite{springer.2019s}, welche die drei charakteristischen Merkmale besonders 
aufgreift:
\begin{quote}
    â€Augmentierte RealitÃ¤t (AR) ist eine (unmittelbare und interaktive) um virtuelle Inhalte (fÃ¼r beliebige Sinne) angereicherte Wahrnehmung der 
    realen Umgebung in Echtzeit, welche sich in ihrer AusprÃ¤gung und Anmutung soweit wie mÃ¶glich an der RealitÃ¤t orientiert, sodass im 
    Extremfall (so dies gewÃ¼nscht ist) eine Unterscheidung zwischen realen und virtuellen (Sinnes-) EindrÃ¼cken nicht mehr mÃ¶glich ist.â€ \cite{springer.2019s}
\end{quote}
Diese Definition nimmt sich als Grundlage die oben aufgefÃ¼hrte Definition nach \cite{azuma.1997a}.
\\ 
\linebreak
Der Author L. Frank Baum \cite{frankbaum.1856m} verkÃ¼ndete die ersten Ideen und Gedanken einer Augmented Reality Anwendung in 
\textit{â€The Master Keyâ€œ} \cite{masterkey.1996f}. Eine erste tatsÃ¤chliche Realisierung eines Augmented Reality Systems erfolgte erst Ã¼ber 
60 Jahr spÃ¤ter. Ivan Edward Sutherland \cite{sutherlandbio.1938m} stellte sein Projekt 1968 an der University of Utah vor. Dabei handelte es 
sich um ein sogenanntes \textit{\ac{HMD}}. Ziel dieser Entwicklung war weniger das Erweitern der RealitÃ¤t, sondern dreidimensionale 
Illusionen zu erzeugen die reale Objekte mit einer einfachen Grafik in Echtzeit Ã¼berlagert. %\cite{display.1965f}
Trotz dessen gilt er als erste Person mit der Vision, einen Nutzer in realer Umgebung mit virtuellen Objekten interagieren zu lassen.
\\ 
Anfang der 90er Jahre prÃ¤gten zwei Forscher, Thomas P. Caudell und David W. Mizell, den Begriff der Augmented Reality durch ein Pilotprojekt
bei Boeing. Das Projekt diente dazu Informationen in das Gesichtsfeld Ã¼ber eine Brille einzusetzen, um Arbeitern das Verlegen von Kabeln im und um das 
Flugzeug zu erleichtern. Nach dieser bahnbrechenden Erfindung begann eine stetige Weiterentwicklung der Technologie. Im Jahre 1999 wurde 
von Hirokazu Kato und Mark Billinghurst \textit{ARToolKit}, ein Computer-Vision-basiertes Tracking fÃ¼r AR, verÃ¶ffentlicht und â€lÃ¶ste eine 
groÃŸe Welle an Forschungsarbeiten auf der ganzen Welt aus.â€œ \cite{springer.2019s} 
\begin{quote}
    We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators 
    are represented on Virtual Monitors which can be freely positioned about a user in space. Users can collaboratively view and interact 
    with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and 
    accurate computer vision techniques and HMD calibration. We propose a method for tracking fiducial markers and a calibration method 
    for optical see-through HMD based on the marker tracking. \cite{artoolkitsheet.1999o}
\end{quote}
Dieser Ausschnitt war der grundlegende Baustein des Durchbruchs dieser Technologie und den vorangestellten Forschungen und eine fundierte 
Grundlage fÃ¼r alle Forschungen und Entwicklungen die darauf folgten. 
\\ 
Heutzutage dreht sich die Entwicklung um mobile AR, welche durch die anfÃ¤ngliche Revolution von \textit{ARToolkit} und die darauf entstehenden 
Entwicklungen und Produktionen von groÃŸen Firmen, wie z.B. Google, Microsoft, Apple und Facebook entstand. Die zuletzt groÃŸe Bewegung in dem 
Bereich der AR waren die Vorstellungen groÃŸer Software-Plattformen fÃ¼r mobile \acs{AR}-Applikationen. Durch \textit{Apple's ARKit} und \textit{Google's ARCore} 
kamen im Jahr 2017 zwei moderne und innovative Frameworks auf den Markt, die die Entwicklung von \acl{AR}-Applikationen stark beeinflussen.
Die Frameworks wurden bei den ersten Produktionen fÃ¼r Entertainment-Anwendungen genutzt, um z.B. mobile Spiele zur Unterhaltung oder 
Funktionen bei Sport-FernsehÃ¼bertragungen zur Anzeige der Entfernung des FreistoÃŸes zu realisieren.
\\ 
\linebreak
Diese Arbeit jedoch widmet sich ausschlieÃŸlich dem industriellen Aspekt und stellt andere Bereiche in den Hintergrund. Die in Kapitel 
\ref{chap:Motivation} aufgefÃ¼hrte Markstudie bestÃ¤tigt das enorme Potential hinter \acl{AR} und deren Einsetzbarkeit in der Industrie.
HauptsÃ¤chlich in der Produktion, der Wartung oder der Reparatur von Maschinen kann Augmentierte RealitÃ¤t eingesetzt werden und zeigt einen 
positiv erzeugten Mehrwert. Dabei kÃ¶nnen bei Maschinen das Anzeigen von protokollierten Fehlern oder eine visuelle Hilfestellung bei Defekts, sowohl bei der 
Reparatur, als auch bei der Ersetzung einzelner Komponenten eine deutliche Reduzierung des zeitlichen Aufwands oder eine effektivere Arbeitsweise 
vorweisen. 
\\
\textit{Harvard Business Review} legte einen Vergleich offen, indem ein Techniker ein SteuergerÃ¤t einer Windkraftanlage mithilfe 
eines \acs{AR}-Headsets verkabelt und in Betrieb nimmt. Alle benÃ¶tigten Informationen wurden Schritt fÃ¼r Schritt Ã¼ber das Headset zur VerfÃ¼gung 
gestellt. Dadurch gab es keinen Mehraufwand, z.B. das Nachschlagen in einer Dokumentation. Danach fÃ¼hrte der Techniker den gleichen Prozess 
ohne die Hilfe der AR-Anwendung durch, lediglich mit Verwendung des vorliegenden Handbuchs, welches physisch beiseite lag.
Dieser Test bestÃ¤tigte eine Leistungsverbesserung des Arbeiters beim ersten Gebrauch um \textit{34\%}.\cite{harvardbr.2017m} Diese Erkenntnis 
des Tests stÃ¼tzte den Gedanken der Leistungsverbesserung und der Reduzierung des zeitlichen Aufwands, somit lies dieses Resultat den 
Anlass zu dieses Ergebnis auf die Gesamtheit zu projizieren und fÃ¼r alle AnwendungsfÃ¤lle allgemeingÃ¼ltig zu machen. 

% https://ntrs.nasa.gov/search.jsp?R=19830003536 
% https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19830003536.pdf 
% https://www.arsoft-company.com/en/dar-project/

\subsection{Virtual Reality, Augmented Reality und Mixed Reality}
WÃ¤hrend immer mehr Leute mit dem Begriff Virtual Reality etwas anfangen kÃ¶nnen, gibt es doch noch viele Unsicherheiten bei den dazukommenden 
Begriffen der Augmented und der Mixed Reality. Diese drei Begriffe lassen sich meist nicht immer voneinander unterscheiden, da es viele 
Ãœberschneidungen aber auch gravierende Unterschiede gibt. 
\\ 
Folgender Abschnitt beleuchtet die Unterschiede und lÃ¤sst die drei Formen der erweiterten RealitÃ¤t voneinander unterscheidbar machen.
\subsection*{Virtual Reality}
Virtual Reality, dt. Virtuelle RealitÃ¤t (\acs{VR}), ist eine in Echtzeit computergenerierte, interaktive und virtuelle Umgebung. Eine Darstellung 
und gleichzeitige Wahrnehmung der Wirklichkeit in all ihren Facetten und Eigenschaften. Das Ziel dieser Technologie ist, den Nutzer von der 
AuÃŸenwelt abzuschirmen und diese durch eine computergenerierte und detaillierte Welt zu ersetzen. \cite{vr.2018n} Auch bekannt als Immersion.
\\ 
\linebreak
Die konventionelle Computergraphik ist fÃ¼r den Menschen spÃ¼rbar nicht von belangen und weckt keine physischen Emotionen, wogegen VR diese 
etwas beeinflussen kann. Wie diese Unterschiede spÃ¼rbar sind, wird in folgendem erlÃ¤utert.
\\  
Die Tabelle \ref{tbl:vrtabelle} fasst die Unterscheidungsmerkmale von Virtueller RealitÃ¤t zur konventionellen Computergraphik zusammen. \cite{springer.2019s}
\begin{table}[!htb]
    \centering
    \begin{tabular}{ll}
      \textbf{3D- Computergraphik}  & \textbf{Virtuelle RealitÃ¤t} \\
      \hline
      Rein visuelle PrÃ¤sentation & Multimodale PrÃ¤sentation \\ %(d. h. mehrere SinnesmodalitÃ¤ten ansprechende also z. B. gleichzeitig visuelle, akustische und haptische)
      \hline
      PrÃ¤sentation nicht notwendigerweise zeitkritisch & Echtzeitdarstellung \\
      \hline
      Exozentrische Perspektive & Egozentrische Perspektive \\
      \hline
      Statische Szene oder vorberechnete Animation & Echtzeitinteraktion und -simulation \\ 
      \hline
      2D-Interaktion (Maus, Tastatur) & 3D-Interaktion (KÃ¶rperbewegung, -gestik) \\ 
      \hline
      Nicht-immersive PrÃ¤sentation & Immersive PrÃ¤sentation \\ 
    \end{tabular}
    \caption{Merkmale der Computergraphik gegenÃ¼ber der VR \cite{springer.2019s}}
    \label{tbl:vrtabelle}
    % Verweis im Text mittels \ref{tbl:vrtabelle}
\end{table}
\\ 
\linebreak 
Virtuelle RealitÃ¤t ist immer in Verbindung mit \acl{HMD}s zu betrachten, da ein GerÃ¤t benÃ¶tigt wird, welches den Nutzer von der realen Welt 
abschottet und in die virtuelle Welt begleitet. Diese werden auf Hochtouren von groÃŸen Firmen, wie Microsoft, Sony, Facebook etc. entwickelt. 
Die erste entwickelte und auf dem Markt verÃ¶ffentlichte Brille war die HoloLens von Microsoft, gefolgt von der Brille Namens Oculus Rift von 
Facebook usw. 
\\ 
Mit der stetigen Weiterentwicklung dieser Brillen und der Technologie wird versucht nach und nach mehr Sinne des Menschen manipulieren zu 
kÃ¶nnen, bzw. das Spiel- und GefÃ¼hlserlebnis bei Konsolen-Spielen immer realistischer zu gestalten. Allerdings sind die MÃ¶glichkeiten im 
Massenmarkt stark beschrÃ¤nkt auf die folgenden aufgelisteten Sinne: 
\begin{itemize}
    \item Sehen: Durch \acl{HMD}s (Oculus Rift), die die reale Welt abschirmen und vom Nutzer nicht mehr wahrgenommen werden kann
    \item HÃ¶ren: Durch KopfhÃ¶rer, somit werden GerÃ¤usche der RealitÃ¤t Ã¼bertÃ¶nt 
    \item FÃ¼hlen: Durch Controller mit haptischem Feedback, um Ereignisse der virtuellen Welt physisch spÃ¼rbar zu gestalten. 
\end{itemize}
Die Entwicklung dieser Technologie wird uns in Zukunft weiter verfolgen. Vielleicht gibt es irgendwann die MÃ¶glichkeit weitere Sinne virtuell 
zu steuern. An den UniversitÃ¤ten von Singapur und Tokio zum Beispiel, gibt es Forscher-Teams die ein groÃŸes Budget zur VerfÃ¼gung haben, um 
dieser SinnestrÃ¼bung auf den Grund zu gehen. Sie fanden heraus, das thermische und elektrische Stimulationen bestimmte GeschmackseindrÃ¼cke 
vermitteln kÃ¶nnen. Auch Wissenschaftler in China und an der Oxford University haben herausgefunden, dass bestimme Audiosignale mit einem sÃ¼ÃŸen 
Geschmack assoziiert werden. \cite{sinnesforschung.2017m} Forscher aus aller Welt gehen mit einer bestimmten Ernsthaftigkeit die MÃ¶glichkeiten 
der Sinneserweiterung in der virtuellen RealitÃ¤t an.   
\subsection*{Augmented Reality}
\ac{AR} setzt im Gegenzug zu \ac{VR} auf das tatsÃ¤chliche Erweitern der RealitÃ¤t durch das Einblenden von Informationen, VorgÃ¤ngen, Hilfestellungen
oder die Wegbeschreibung bei Head-Up Displays in Personen-, Kraft- und Nutzfahrzeugen, wÃ¤hrend \acl{VR} den Nutzer in eine vÃ¶llig eigene 
Welt entlockt und von der RealitÃ¤t abkapselt. Bei \acs{AR} soll dem Nutzer die zu bewÃ¤ltigenden Aufgaben und dessen Bestreben vereinfacht und 
gestÃ¼tzt werden ohne die RealitÃ¤t auÃŸer Acht zu lassen. DarÃ¼berhinaus bietet \acl{AR} deutlich mehr AnsatzmÃ¶glichkeiten diese Technologie 
umzusetzen und zu nutzen, z.B. wie erwÃ¤hnt durch Head-Up Displays bei Autos und Flugzeugen, durch Smartphones und Tablets die durch die Kamera 
die RealitÃ¤t erweitern, Brillen mit eingeblendeten Projektionen, wie die Oculus Rift und anderweitige groÃŸe Projektionen. 
\\ 
In Kapitel \ref{subchap:Varianten der AR} wird auf die Varianten genauer eingegangen. 

\subsection*{Mixed Reality}
Dem Namen entsprechend ist \ac{MR} eine Mischung aus \acl{AR} und \acl{VR}, jedoch die Art und Weise ist dabei entscheidend. Es gibt \acs{MR}
-Brillen, die die reale Welt als Basis der Interaktion nehmen und Brillen die alleinig auf digitalen Bildern aufbauen. \cite{mr.2018o}
\subsubsection*{Mixed Reality bezÃ¼glich der realen Welt}
Diese Art der erweiterten RealitÃ¤t basiert auf den gegebenen Grundlagen der \acs{AR}. Ã„hnlich zu \acl{AR} werden Objekte der realen Welt 
hinzugefÃ¼gt, indem sie an bestimmte Stellen projiziert werden. \acl{MR} baut darauf auf und verankert die digitalen Objekte mit dem realen
dreidimensionalen Raum, sodass eine RealitÃ¤tsnahe Interaktion stattfinden kann. Virtuelle und echte Welt verschmelzen dadurch endgÃ¼ltig zu 
einer einzigen Welt. 

\subsubsection*{Mixed Reality bezÃ¼glich der virtuellen Welt}
Hinsichtlich der \acs{VR} gibt es zu Anfang keine grundlegenden Ã„nderungen gegenÃ¼ber der \acs{MR}. Hierbei kann der Nutzer durch 
\acs{MR} ebenso die AuÃŸenwelt ausblenden und sich lediglich auf die virtuelle Wahrnehmung fixieren. Erst in der Benutzung werden die Unterschiede 
zu \acs{VR} sichtlich und dieser ist gravierend. Der Nutzer kann sich frei bewegen, d.h. die Bewegungsfreiheit wird nur durch die reale Umgebung 
eingeschrÃ¤nkt, wÃ¤hrend \acl{VR} sich auf einen sensorgestÃ¼tzten Raum begrenzt. \cite{vr.2018n}
\\ 
Bei \acs{MR} wird jeder Schritt und jede Bewegung in die computergenerierte Welt Ã¼bertragen und schafft so deutlich mehr Bewegungsfreiheiten.
\\ 
\linebreak 
Ein groÃŸer Investor dieser Technologie ist Microsoft mit \ac{WMR}, wobei bereits schon Erfolge erzielt werden konnten. Eine Standhafte Plattform 
die nur darauf wartet ausgebaut und intensiver genutzt zu werden. FÃ¼r die Plattform vorgesehene Brillen gibt es schon viele Produzenten, wie z.B. 
die \textit{HMD Odyssey} von Samsung oder der \textit{Explorer} von Lenovo.

\subsection{Varianten der Augmented Reality}
\label{subchap:Varianten der AR}
Augmented Reality Anwendungen funktionieren alle nach dem gleichen Prinzip und verfolgen das gleiche Ziel, die RealitÃ¤t durch digitale Informationen 
oder Objekte zu erweitern. Die einzige deutliche Abweichung ist das jeweilige EndgerÃ¤t und die technische Umsetzung in Zusammenhang mit der 
Hardware. Auf zwei dieser Varianten, mobiles und Smart-Brillen- oder auch Headset-AR genannt, wird in Folgendem eingegangen.

\subsection*{Mobiles AR}
Smartphones sind in unserer Gesellschaft nicht mehr wegzudenken und haben sich fest etabliert. Durch die vielseitige und alltÃ¤gliche Nutzung 
von Tablets und Smartphones erÃ¶ffnete diese Sparte eine gute MÃ¶glichkeit \acl{AR} in den Alltag zu integrieren. Somit beleibt der stÃ¤ndige 
Gebrauch dieser Technologie nicht aus und bereichert die Art und Weise Spiele und Anwendungen zu entwickeln und zeigt den Fortschritt der technologischen 
Aspekte sowohl software-, als auch hardware-technisch.
Durch die im Smartphone integrierte Kamera werden Live-Aufnahmen analysiert und dienen als Ausgangssituation fÃ¼r die \acs{AR}-Anwendung. Durch 
die gegebene MÃ¶glichkeit der vorhandenen Kamera, kann ein Echtzeithintergrund erzeugt und zusÃ¤tzlich mit Informationen per Overlay dargestellt 
werden. Je nach Anwendung kann der Nutzer auf verschiedene Weise mit den eingeblendeten und virtuellen Objekten interagieren, z.B. durch das 
Bewegen des GerÃ¤ts, um das Objekt aus verschiedenen Blickwinkeln anschauen zu kÃ¶nnen, oder der direkten Interaktion mit dem Objekt durch 
Drehen, Skalieren oder Verschieben am Bildschirm. Ein Vorreiter der mobilen AR ist das 2016 auf dem Markt erschienenen \textit{PokÃ©mon Go} 
von Niantic, das den Ansatz der \acs{AR} prÃ¤gt. \cite{pokemongo.2016a}
\\ 
\linebreak
Die weit verbreitete social Media Applikation \textit{Snapchat} baut seit geraumer Zeit ebenso auf \acl{AR}, um Bilder lebhafter zu 
gestalten, wie der folgenden Abbildung \ref{pic:snapchatAR} zu entnehmen ist. 
\\ 
\linebreak 
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=10cm,height=7.5cm,keepaspectratio]{2Grundlagen/Bilder/snapchatAR.jpeg}
    \caption{Mobile-AR in Snapchat}
    \label{pic:snapchatAR}
\end{figure}
\subsection*{Smart-Brillen AR}
Unter Smart- oder Datenbrillen und Smart Glasses wird ein Konstrukt verstanden, das eine Brille mit einem tragbaren Minicomputer verwirklicht. 
Dabei werden Informationen Ã¼ber kleinste Monitore oder Prismen ausgegeben und dem Nutzer zusÃ¤tzlich in das Sichtfeld projiziert. Im Gegensatz
zur mobilen \acs{AR} hat der Nutzer kein GerÃ¤t in der Hand und ist somit in seiner Bewegung weniger eingeschrÃ¤nkt und flexibler. 
\\ 
\linebreak
Die Technologie der Smart-Brillen ist ein Ã¤hnliches Konzept zu der \acs{VR}-Brille, allerdings befindet sich die Smart-Brille fÃ¼r den 
Ã¶ffentlichen Gebrauch noch in der Entwicklungsphase, da nur bedingte Funktionen mÃ¶glich sind und die Kosten fÃ¼r den Normalverbraucher nur 
bedingt tragbar erscheinen. Mit der Ã¼berarbeiteten Version der \textit{HoloLens 2} von Microsoft ist eine deutlich komfortablere 
MÃ¶glichkeiten des alltÃ¤glichen Gebrauchs geboten, da diese im Vergleich zum VorgÃ¤ngermodell deutlich komfortabler, leichter und 
handlicher ist. Das Design der Datenbrille ist von einer normalen Brille noch weit entfernt, ermÃ¶glicht mittlerweile aber das 
uneingeschrÃ¤nkte Sehen und Wahrnehmen der Umgebung. Die Bedienung des GerÃ¤ts basiert auf schon vorhandenen MÃ¶glichkeiten die bereits in anderen 
GerÃ¤ten Anwendung finden. Darunter gibt es am GerÃ¤t angebrachte Touchsensoren, Handbewegungen und -gesten und Eye-Tracking. 
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=10cm,height=7.5cm,keepaspectratio]{2Grundlagen/Bilder/smartglassAW.png}
    \caption{Test der HoloLens 2}
    \label{pic:testholo}
\end{figure}
\\ 
\linebreak 
Speziell im industriellen Bezug bieten die neuentwickelten Brillen ein komfortables und akzeptables Gewicht, sodass diese ohne groÃŸe 
Probleme dauerhaft tragbar sind und den Mitarbeiter bei seiner Arbeit nicht Ã¼bermÃ¤ÃŸig einschrÃ¤nken.
\\ 
\linebreak
Die Programmierung solcher \acs{AR}-Brillen laufen hÃ¤ufig Ã¼ber eigene \acs{SDK}s und plattformunabhÃ¤ngige Laufzeit- und 
Entwicklungsumgebungen, z.B. Unity oder Unreal Engine. Diese gelten als fÃ¼hrende Produkte im Bereich der 3D-Echtzeitdarstellung.
Ein marktfÃ¼hrendes Produkt ist unter anderem die Microsoft HoloLens 2, die der folgenden Abbildung zu entnehmen ist. 
\begin{figure}[hbt!]
    \centering
    \subfigure[Google Glass 2]{\includegraphics[width=7.5cm,height=5cm,keepaspectratio]{2Grundlagen/Bilder/googleglass.png}}
    \subfigure[HoloLens 2]{\includegraphics[width=7.5cm,height=5cm,keepaspectratio]{2Grundlagen/Bilder/hololensside.png}}
    \caption{Datenbrillen (\acs{HMD})}
    \label{pic:datenbrillen}
\end{figure}
\\ 
\linebreak
Eine schwer zu umgehende Herausforderung der \acl{AR} ist die Bestimmung der Position in der Daten und Objekte projiziert werden. Auf die 
AnsÃ¤tze, diese Herausforderung zu bewÃ¤ltigen, wird in folgendem Abschnitt \ref{sec:posi} eingegangen.
\subsection{Positionsbestimmung}
\label{sec:posi}
Um ein digitales Objekt als Overlay dem Kamera-Live-Bild hinzuzufÃ¼gen, werden genauestens definierte Positionen benÃ¶tigt. Diese Positionen 
kÃ¶nnen durch unterschiedliche AnsÃ¤tze ermittelt werden. Je nach Anwendungsfall, z.B. als Navigation, Routenplaner oder Google Maps 
\textit{â€Live Viewâ€œ} \cite{googleliveview.2019a}, reicht eine etwas ungenauere Positionsbestimmung per \acs{GPS}, da in Relation zur 
realen Welt eine Abweichung um Zentimeter oder wenige Meter nicht von belangen ist. Bei Positionsbestimmungen auf kleinstem Raum ist eine 
genaue Lokation wichtig und basiert auf einem deutlich prÃ¤ziseren Ansatz. 
\\ 
Welche oben genannten AnsÃ¤tze es gibt und welche Unterschiede zu beachten sind, wird in Folgendem nÃ¤her darauf eingegangen. 
\subsubsection*{Marker-basierte Positionsbestimmung}
Speziell bei der Marker-basierten Positionsbestimmung gibt es verschiedene MÃ¶glichkeiten den Marker zu gestalten. Es kÃ¶nnen 
BinÃ¤r- oder QR-Codes als Markierung verwendet werden, ein Beispiel eines solchen Codes ist der Abbildung \ref{pic:markerARpos} zu entnehmen. 
Diese Codes sind meistens quadratisch und haben ein eindeutiges Zeichen in der Mitte. Um die Rechenzeit gering zu halten gibt es einfache Muster, 
wie die Abbildung \ref{pic:markerARpos} zeigt.
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=5cm,height=5cm,keepaspectratio]{2Grundlagen/Bilder/bildmarkerAR.png}
    \caption{Marker-basierte Augmented Reality Positionsbestimmung}
    \label{pic:markerARpos}
\end{figure}
Neben der einfachen Markierungserkennung gibt es eine weiterentwickelte MÃ¶glichkeit der Bild- sowie der Objekterkennung. Diese sind AnsÃ¤tze 
die grundlegend auf dem Ausgangspunkt des BinÃ¤r-Code-Verfahrens aufbauen. Eine detailliertere ErlÃ¤uterung der soeben genannten 
ErkennungsmÃ¶glichkeiten findet im Rahmen dieser Arbeit nicht statt. Allerdings wird allgemein kurz auf die Funktionsweise einer 
Marker-basierten Positionsbestimmung eingegangen.
\\ 
Durch ein Kamerabild wird nach einem vordefinierten Marker, bzw. nach einer festgelegten Markierung gesucht. Ist diese mit der Kamera erfasst, 
wird die Markierung durch Bildverarbeitungsalgorithmen und bestimmter Filterung eindeutig identifiziert. Mit den gewonnen Informationen und 
der Ãœbereinstimmung des angegebenen Codes wird die Position, sowie die Orientierung des Markers berechnet. Mit den Angaben der Lage und 
Orientierung wird auf der Markierung das anzuzeigende digitale Objekt generiert und als Overlay Ã¼ber dem Kamerabild angezeigt. Die Markierung 
dient sozusagen als Grundlage um den digitalen Gegenstand Ã¼berhaupt anzeigen zu kÃ¶nnen.
\\ 
Da der Marker immer im Blickfeld der Kamera sein muss, um das virtuelle Objekt anzuzeigen, bringt diese Art der Positionsbestimmung eine 
enorme EinschrÃ¤nkung mit sich, die in diesem Bezug unumgÃ¤nglich ist. 
\\ 
\linebreak
Ein weiterer Ansatz der Positionsbestimmung ist als Ãœberbegriff das GegenstÃ¼ck zur oben aufgefÃ¼hrten Lokalisierung von Markern, die 
sogenannte Marker-unabhÃ¤ngige Positionsbestimmung, welche ebenso verschiedene AusfÃ¼hrungen vorweist. 
\subsubsection*{\acs{GPS}-basierte Positionsbestimmung}
Die Methode des \acs{GPS}-basierten Positionsbestimmungsverfahren verwendet hauptsÃ¤chlich die Koordinaten der realen Welt. In Zusammenarbeit 
mit zusÃ¤tzlichen im AnwendungsgerÃ¤t verbauten internen Sensoren, bspw. Positions-, Geschwindigkeits- und Beschleunigungssensoren und Teile des 
\ac{INS}, z.B. dem Gyroskop, kann diese Art der Positionsbestimmung optimal Anwendung finden. Allerdings werden dabei deutlich mehr Komponenten
benÃ¶tigt und ist deutlich komplexer umzusetzen, als ein markerbasiertes System. 
\\ 
Bei einem Anwendungsfall von \acs{GPS}-basierter Positionsbestimmung geht es meist um Routenplaner, Navigation oder Szenarien die sich 
auf offenen FlÃ¤chen abspielen, da im Gegensatz zu Marker-basierten Anwendungen das GrÃ¶ÃŸenverhÃ¤ltnis deutlich gerÃ¤umiger ist. Der Benutzer ist 
nicht auf einen bestimmten Bezirk beschrÃ¤nkt und ist nicht auf die millimetergenauer Darstellung angewiesen.
\\ 
\linebreak
Eine Alternative zu \acs{GPS} ist die Anwendung des \acs{SLAM}-Verfahrens. Dabei wird eine virtuelle Karte, bzw. ein geometrischen Modell der 
Umgebung erstellt. Wichtige Grundlagen zum Erzeugen eines solchen Modells sind eigenstÃ¤ndig gefundene Landmarken die gleichzeitig 
lokalisiert werden. Darauf folgt ein Vergleich der Pose, der Position des GerÃ¤ts und der geschÃ¤tzten Karteninformationen aus dem Scan der 
Umgebung. 
Damit sind im erweiterten Sinne Marker geschaffen, die Anhaltspunkte fÃ¼r \acl{AR}-Interaktionen schaffen.
\\ 
In Kapitel \ref{chap:SLAM} wird die Thematik des \acs{SLAM}-Verfahrens genauestens erlÃ¤utert. 

\subsection{Augmented Reality in der Industrie}
Das weit gefÃ¤cherte Portfolio der \acl{AR} umfasst viele Anwendungsbereiche und Fachgebiete. Selbst dem Ã¼bergeordneten Bereich der Industrie 
gibt es viele verschiedene Einsatzgebiete. Aus den vielen MÃ¶glichkeiten der Anwendung haben sich Ã¼ber die Jahre der Entwicklung der Technologie 
einige Gebiete in der Industrie herauskristallisiert, die besonders hohen Nutzen stiften. \cite{einsatzgebietear.2017a} In den Bereichen 
Instandhaltung und Wartung, Betrieb und Training ist \acl{AR} auf bestem Wege fester Bestandteil des Alltags zu werden. Bei dieser Betrachtung 
ist es sinnvoll sich auf die damit einhergehenden LÃ¶sungen zu fokussieren, die Reduktion der Kosten und dem Zeitaufwand, sowie die Verbesserung 
der Sicherheit. \cite{studieptc.2020j} 
\\ 
\linebreak
Bei einer Wartung oder Reparatur einer Maschine sind notwendige Informationen direkt greifbar und werden Schritt fÃ¼r Schritt angezeigt, sodass 
in bestimmten Situationen selbst ein Leihe die Anweisungen befolgen kÃ¶nnte. So werden zusÃ¤tzliche Recherchearbeiten oder Unklarheiten Ã¼ber 
VorgÃ¤nge aus dem Weg gerÃ¤umt. Ein Mitarbeiter kann so mit einem Tablet oder einer Smart-Glas Anweisungen visuell auf die realen Maschinen 
projizieren und Arbeitsschritte im Sichtfeld anzeigen lassen. Ebenso geht dieser Vorgang auch bei der Produktion von Bauteilen o.Ã¤., indem 
eine \acs{AR}-Anwendung Anweisungen und Prozessschritte, z.B. auf das WerkstÃ¼ck oder Produkt, projiziert.
\\ 
Die Inbetriebnahme oder Bedienung einer komplexen Anlage oder Maschine ist nach herkÃ¶mmlichen Standard enorm zeitintensiv und dadurch kÃ¶nnen
zusÃ¤tzlich viele Fragen aufkommen, die meist den Prozess noch lÃ¤nger gestalten als vorgesehen. \acs{AR}-Anwendungen kÃ¶nnen 
Bedienungsanleitung oder -hilfen digitalisiert, indem Informationen, Inhalte oder Bedienelemente durch \acl{AR} auf der Anlage platziert werden. 
\\ 
In Fortbildungen, Schulungen oder Einarbeitungen in neue GerÃ¤te kann \acs{AR} durchaus von groÃŸem Vorteil sein. Mit wenig Aufwand, einem 
effektiveren Training kÃ¶nnen Schulungen interaktiver und vor allem sicherer abgehalten werden. Durch die Visualisierung der Trainings- und 
Schulungsinhalten verbessert sich der Lernprozess und damit wird auch die Nutzung der Maschinen verstÃ¤ndlicher. \cite{einsatzgebietear.2017a}

\section{SLAM - Simultanious Localization And Mapping}
\label{chap:SLAM}
\begin{quote}
    Das Simultaneous Localization and Mapping oder kurz SLAM Problem behandelt das gleichzeitige SchÃ¤tzen der Position und Ausrichtung einer 
    mobilen Plattform im Raum anhand der sich an Bord befindlichen Sensoren sowie den Aufbau eines Modells der Umgebung. Dieses Problem ist 
    von groÃŸer praktischer Relevanz und ist Kernbestandteil der meisten mobilen Sensor- systeme. \cite{slamdefi.2016a}
\end{quote}
Als Simultaneous Localization and Mapping auch \acs{SLAM}-Problem gennant, bezeichnet man die Aufgabe, die Trajektorie \footnote{LÃ¶sungskurve oder Bewegungspfad eines Objekts} 
samt Orientierungsinformation einer sich bewegenden Plattform, z.B. ein Smartphone, Tablet oder jegliche Art von Roboter, aus 
Beobachtungen zu schÃ¤tzen und gleichzeitig aus den gewonnenen Informationen eine Karte der Umgebung zu erstellen.
Diese Aufgabe ist fÃ¼r den weiteren Prozess von hoher Bedeutung. Zum einen sollen die generierten Karten sehr prÃ¤zise sein, um einen hohen 
Wert fÃ¼r den Nutzer oder fÃ¼r spezielle Anwendungen, die auf der Karte aufbauen, darzustellen. Zum anderen benÃ¶tigen autonome Roboter, 
beispielsweise Saug- oder MÃ¤hroboter, ein solch erzeugtes geometrisches Modell der Umgebung, um zielgerichtet selbststÃ¤ndig navigieren zu 
kÃ¶nnen. \cite{slamdefi.2016a} 
\\ 
\linebreak
1986 wurden auf der \textit{IEEE Robotics and Automation Conference} erste mathematische Definitionen vorgenommen, die mittels statischer 
Theorien ermittelt und mit ersten Studien belegt wurden. Einige Jahre spÃ¤ter,im Jahr 1995, wurde das \acs{SLAM} Problem erstmals auf dem 
internationalen Symposium fÃ¼r Robotikforschung (\textit{ISRR'95}) vorgestellt. Die Forschungen hielten an, bis auf der \textit{ISRR'99} 
die erste \acs{SLAM} Sitzung stattfand. 
\subsection{Definition des Problems}
Angenommen ein Roboter startet in einer Position auch Pose gennant und Konfiguration \textit{p0} und bewegt sich durch eine ihm 
unbekannte Umgebung. Die Einstellung beinhaltet Position und Ausrichtung des Roboters. Je nach Bewegung in Raum oder Ebene ist die Pose 
meist als 3- oder 6-dimensionaler Vektor abgebildet. Die Bewegung des Roboters wird durch bekannte Kontrollkommandos \textit{u} angewiesen, 
allerdings mit einer gewissen Unsicherheit versehen. Dabei wird zwischen den Zeitpunkten \textit{t - 1} und \textit{t} die Bewegung des 
Roboters mit \textit{ut} beschrieben und somit auch die unterschiedlichen Posen \textit{pt-1} nach \textit{pt}. Die Umgebung wird parallel 
dazu Ã¼ber diverse Sensoren, z.B. interne Sensoren, bspw. Gechwindigkeits- oder Positionssensoren, und externer Senoren, unter anderem 
Abstands- oder taktile Sensoren. Neben der Protokollierung der Position, bei der es zu StÃ¶rungen oder Berechnungsfehler kommen kann, gibt 
es Beobachtungen durch Sensoren die verrauscht \textit{zt} sind.
\\ 
\linebreak
Mit diesen vorhandenen Werten ist die Zielsetzung die SchÃ¤tzung der Trajektorie ... des Roboters von Beginn der Fortbewegung bis zum 
Zeitpunkt \textit{T}.

% Wir kÃ¶nnen das SLAM Problem wie folgt definieren: Eine mobile Sensorplattform startet in der Konfiguration   ğ‘0  und bewegt sich durch 
%eine unbekannte Umgebung. Die Konfiguration beinhaltet dabei die Position sowie die Ausrichtung der Plattform und wird oft als Pose 
%bezeichnet. Die Pose ist meist ein 3- oder 6-dimensionaler Vektor, je nach dem, ob die Bewegung in der Ebene oder im Raum stattfindet. 
%Die einzelnen Bewegungen der Plattform werden durch bekannte Kontrollkommandos   ğ‘¢  angewiesen, allerdings sind sÃ¤mtliche Aktionen mit 
%Unsicherheit behaftet. Dabei beschreibt   ğ‘¢ğ‘¡  die Bewegung der Plattform zwischen den Zeitpunkten t âˆ’ 1 und t und somit die Bewegung von  
%ğ‘ğ‘¡âˆ’1  nach   ğ‘ğ‘¡ . Gleichzeitig wird die Umgebung durch einen Sensor wahrgenommen. Wie die Bewegungen ist auch jede einzelne Beobachtung   
%ğ‘§ğ‘¡  verrauscht.

%Das Ziel ist nun die SchÃ¤tzung der Trajektorie   ğ‘0:ğ‘‡=[ğ‘0,ğ‘1,â€¦,ğ‘ğ‘‡]ğ–³  der Plattform vom Beginn der Bewegung bis zum Zeitpunkt T. 
%ZusÃ¤tzlich wird eine Karte   ğ‘š  der Umgebung geschÃ¤tzt, deren ReprÃ¤sentation allerdings flexibel sein kann. Eine solche Karte kann 
%beispielsweise aus einzelnen Punkten bestehen, sie kann aber auch ein OberflÃ¤chenmodell, eine 2D-Rasterkarte oder 3D-Voxelkarte sein. 
%Zur SchÃ¤tzung von   ğ‘0:ğ‘‡  und   ğ‘š  wird die Wahrscheinlichkeitsverteilung   ğ‘(ğ‘0:ğ‘‡,ğ‘šâˆ£ğ‘§1:ğ‘‡,ğ‘¢1:ğ‘‡)  berechnet, d.â€‰h. es geht um die Bestimmung 
%aller Posen sowie der Karte der Umgebung basierend auf der Sequenz von Sensormessungen   ğ‘§1:ğ‘‡  und Kontrollkommandos   ğ‘¢1:ğ‘‡ .

%Neben der Berechnung von   ğ‘(ğ‘0:ğ‘‡,ğ‘šâˆ£ğ‘§1:ğ‘‡,ğ‘¢1:ğ‘‡) , welches hÃ¤ufig als Offline-SLAM bezeichnet wird, ist man in der Praxis hÃ¤ufig an der 
%SchÃ¤tzung der aktuellen Pose x t sowie der Karte interessiert, d.â€‰h.,   ğ‘(ğ‘ğ‘¡,ğ‘šâˆ£ğ‘§1:ğ‘¡,ğ‘¢1:ğ‘¡) . Diese Variante wird oft als Online-SLAM 
%bezeichnet und ist von zentraler Bedeutung, wenn beispielsweise ein mobiler Roboter Entscheidungen basierend auf dem aktuellen 
%Umgebungsmodell bzw. der aktuellen PositionsschÃ¤tzung treffen soll. Dies ist entscheidend fÃ¼r die autonome Navigation oder Exploration.


\subsection{Localization}
\subsection{Mapping}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% -----------> tbd... !! <-----------------




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quaternionen}
\label{chap:Quaternionen}
\subsection{Transformation}
\subsection{Rotation}

\section{Technologien}
\label{chap:Technologien}
Dieser Abschnitt befasst sich mit den wichtigsten Programmier-Technologien, die in diesem Projekt eingesetzt werden. 
%Ein kurzer Ãœberblick 
%Ã¼ber die Historie der Technologien, worum es dabei geht und welche Aufgaben sie umfassen. 
\\ 
Der Augenmerk liegt auf dem Framework \textit{Google ARCore}, da dieses Hauptbestandteil des Projekts ist. 
\subsection{Google ARCore}
ARCore ist die von Google verÃ¶ffentlichte Plattform zum Erstellen von \acl{AR} Erlebnissen, primÃ¤r entwickelt fÃ¼r Android Phones. 
Das \ac{SDK} ist in den Programmiersprachen Java und Kotlin geschrieben. Mit bestimmten \acs{API}s ermÃ¶glicht \textit{ARCore} die Erfassung 
der Umgebung durch ein Tablet oder Smartphone, um so die Welt zu verstehen und mit Informationen zu interagieren. Mitten des Jahres 2017 
wurde die \textit{Google ARCore API} zum ersten Mal den Entwicklern und der \acs{AR}-Community prÃ¤sentiert. Im ersten Quartal 2018 wurde 
diese dann verÃ¶ffentlicht und lÃ¶ste das Vorreiterprojekt \textit{â€Project Tangoâ€œ} ab. 
\\ 
\linebreak 
\textit{Project Tango} war ebenso eine Google-Plattform, mit dem Smartphones und Tablets ein \textit{â€RaumgefÃ¼hlâ€œ} erhalten. \cite{projecttango.2016j} 
Die Plattform kombinierte die Eingabe verschiedener Sensoren, z.B. radarÃ¤hnliche Infrarotstrahler, einer Infrarotkamera und hochprÃ¤zisen 
Beschleunigungsmessern, Barometern und Gyroskopen, um schnell nutzbare Informationen der Umgebung zu generieren. Mit der Ãœberarbeitung des 
Konzepts wurde das neue unabhÃ¤ngige Projekt \textit{ARCore} geschaffen und somit die Anzahl der Hardwarekomponenten reduziert, bspw. wird 
keine spezielle Infrarotkamera mehr benÃ¶tigt. Die ARCore \acs{API} verwendet bei Anwendungen ausschlieÃŸlich das Kamerabild des Smartphones und 
dessen Sensoren. 
\\ 
\linebreak 
Zum Integrieren von virtuellen Objekten in die reale Welt verwendet ARCore drei SchlÃ¼sselfunktionen unter Benutzung von Java und OpenGL, Unity 
und Unreal:
\begin{itemize}
    \item Motion tracking, dt. Bewegungsverfolgung, ermÃ¶glicht das Verstehen und Verfolgen der Postion des GerÃ¤ts relativ zur RealitÃ¤t.
    \item Environmental understanding, dt. VerstÃ¤ndnis der Umgebung, erkennt die Position und GrÃ¶ÃŸe aller Arten von OberflÃ¤chen: horizontale, 
    vertikale und abgewinkelte OberflÃ¤chen, wie WÃ¤nde, BÃ¶den, Tische oder StÃ¼hle.
    \item Light estimation, dt. LichtschÃ¤tzung, schÃ¤tzt die durch das Smartphone gegebenen LichtverhÃ¤ltnisse der Umgebung. \cite{arcorefundamentals.2018m}
\end{itemize}
\subsubsection*{Motion tracking}
FÃ¼r die Funktion der Bewegungsverfolgung verwendet ARCore das Verfahren von \acl{SLAM} (\acs{SLAM}) \ref{chap:SLAM}, um so zu verstehen wo 
sich das Smartphone in Relation zur realen Welt befindet. ZusÃ¤tzlich erkennt ARCore visuell unterschiedliche Merkmale im aktuell aufgenommenen 
Kamerabild, sogenannte Merkmalspunkte \textit{â€feature pointsâ€œ}. Anhand dieser Punkte wird die Ã„nderung des Standortes berechnet. Durch TrÃ¤gheitsmessungen 
der \ac{IMU}, dt. Inertiale Messeinheit des GerÃ¤ts, kombiniert mit den visuellen Informationen des Kamerabildes wird die Position und 
Ausrichtung der Kamera relativ zur RealitÃ¤t abgeschÃ¤tzt. 
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=15cm,height=5cm,keepaspectratio]{2Grundlagen/Bilder/motionTracking.png}
    \caption{Skizze zur Bewegungsverfolgung des Smartphones \cite{arcoreofficial.2020j}}
    \label{pic:motiontracking}
\end{figure}
\\ 
Die \acl{IMU} in Smart-Devices besitzt drei Beschleunigungssensoren, z.B. piezoelektrische Beschleunigungssensoren oder \ac{MEMS}, fÃ¼r die 
drei Raumachsen:
\\ 
\begin{itemize}
    \item Die Abszissenachse (X-Achse), die horizontale (waagerechte) Koordinatenachse,
    \item Die Ordinatenachse (Y-Achse), die darauf vertikale (senkrechte) Koordinatenachse und
    \item Die Applikatenachse (Z-Achse), die auf beiden anderen Achsen senkrechte Achse \cite{koordinatennorm.1973m}
\end{itemize}
Ebenso besitzt die \acs{IMU} Drehratensensoren, z.B. ein Gyroskop zur Messung der Rotationsgeschwindigkeit, welche die Drehraten um 
diese Raumachsen messen. Durch die kontinuierliche Auslesung der Sensordaten, wird die Position des GerÃ¤ts berechnet und in Form von 
drei Koordinaten ausgegeben. Ausgehend von dem Referenzkoordinatensystem wird die Position neu bestimmt. Die Abbildung \ref{pic:Positionsberechnung} 
veranschaulicht solch ein Beispiel.
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=10cm,height=10cm,keepaspectratio]{2Grundlagen/Bilder/imu-Bsp.png}
    \caption{Grundprinzip einer IMU \cite{imubild.2020j}}
    \label{pic:Positionsberechnung}
\end{figure}
\subsubsection*{Environmental understanding}
Die zweite SchlÃ¼sselfunktion des Frameworks ist das VerstÃ¤ndnis der Umgebung. Dieses wird erlangt, indem \textit{â€feature pointsâ€œ}, Ebenen und 
FlÃ¤chen beim scannen des Umfelds erkannt werden. Beim Entstehen mehrerer solcher \textit{feature points} erstellt ARCore ein Cluster aller 
naheliegenden Punkte. Dieses erzeugte Cluster reprÃ¤sentiert eine horizontale oder vertikale FlÃ¤che. Mit diesen gewonnenen Informationen ist das 
Platzieren von virtuelle Objekte auf den erfassten FlÃ¤chen mÃ¶glich.
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=15cm,height=5cm,keepaspectratio]{2Grundlagen/Bilder/environmentalUnderst.png}
    \caption{Skizze zum UmgebungsverstÃ¤ndnis des Smartphones \cite{arcoreofficial.2020j}}
    \label{pic:environmentalunderst}
\end{figure}

\subsubsection*{Light estimation}
Mit der Funktion der LichtschÃ¤tzung kann ARCore LichtverhÃ¤ltnisse und Informationen Ã¼ber die Beleuchtung der Umgebung erkennen. Dadurch kann das 
\acs{SDK} durchschnittliche IntensitÃ¤t und eine angepasste Farbkorrektur eines bestimmten Kamerabildes liefern, um so die Aufarbeitung des Bildes 
der Umgebung anzupassen. Diese Funktion verstÃ¤rkt die Anpassung des virtuellen Objektes an die RealitÃ¤t und lÃ¤sst dieses noch glaubwÃ¼rdiger 
erscheinen.
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=15cm,height=5cm,keepaspectratio]{2Grundlagen/Bilder/lightEstim.png}
    \caption{Skizze zu den LichtverhÃ¤ltnissen des Smartphones \cite{arcoreofficial.2020j}}
    \label{pic:lightestim}
\end{figure}

\subsection{Android Jetpack}
\label{sec:androidjetpack}
Android Jetpack ist eine Ansammlung an Bibliotheken, die es Entwicklern ermÃ¶glicht \textit{Best Practices} zu befolgen, 
Boilerplate\footnote{Sind Codeabschnitte, die an mehreren Stellen mit wenigen oder gar keinen Ã„nderungen aufgefÃ¼hrt sein mÃ¼ssen. \cite{boilerplate.2019m}}-Code 
zu reduzieren und Code zu schreiben. Die durch Android Jetpack zur VerfÃ¼gung gestellten Bibliotheken fÃ¶rdern die konsistente Funktionsweise der 
Android-Versionen und -GerÃ¤ten. DarÃ¼ber hinaus wird mit Android Jetpack ein Ãœberblick gewÃ¤hrleistet, um Best Practices und empfohlene 
Architekturen zu berÃ¼cksichtigen.
\\ 
AuÃŸerdem wurde Android Jetpack fÃ¼r moderne Design-Praktiken entwickelt, darunter fÃ¤llt \textit{â€seperation of concernsâ€œ} 
\footnote{(SoC), Entwurfsprinzip, um verschiedene Verantwortlichkeiten auf verschieden Abschnitte zu trennen. \cite{soc.2019a}}, \textit{testability}, 
\textit{loose coupling}, \textit{Observer Pattern} \footnote{Beobachter-Muster, Verhaltensmuster der GoF-Muster}, 
\textit{Inversion of Control} 
\footnote{(IoC), Umsetzugsparadigma, das ein spezifisches Objekt Ã¼ber ein mehrfach verwendetes Objekt aufruft. \cite{ioc.2020}} und 
\textit{productivity features}, z.B. Plugin-Integrationen. Dies sind Techniken, um eine robuste und qualitativ hochwertige App zu erstellen.
\\ 
Die Bibliothekensammlung ist die Grundlage fÃ¼r die verwendete Architektur, Android Architecture Components in Abschnitt \ref{chap:AAC}.
\\ 
\linebreak
Die Abbildung \ref{pic:androidjetpackcomp} verschafft einen Ãœberblick Ã¼ber die in Jetpack enthaltenen Bibliotheken. % in Abschnitt \ref{chap:AAC}
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=15cm,height=10cm,keepaspectratio]{2Grundlagen/Bilder/androidjetpackcomps.png}
    \caption{Komponenten von Android Jetpack \cite{androidjpdescr.2018m}}
    \label{pic:androidjetpackcomp}
\end{figure}
\\
Eine nÃ¤here ErlÃ¤uterung aller Bibliotheken findet im Rahmen dieser Ausarbeitung nicht statt, wobei die zur Verwendung geplanten 
â€Architecture Componentsâ€œ beschrieben werden. 
\\ 
\linebreak
LiveData, Room und ViewModel sind Bestandteil der \textit{Android Architecture Components}-Konstellation 
und bilden die Basis fÃ¼r die Architektur, welche an das \textit{MVVM-Pattern} angelehnt ist.

\subsection*{LiveData}
\label{sec:LiveData}
\textit{LiveData} gehÃ¶rt zu den \textit{Observer-Pattern} und ist eine Datenhalterklasse die eine Beobachtungsfunktion, bzw. -charakter besitzt. Der 
Unterschied zu einem regulÃ¤ren Observable ist, dass die LiveData-Komponente lebenszyklusunabhÃ¤ngig ist. Dies bedeutet, der Lebenszyklus anderer 
Komponenten z.B. \textit{Activities, Fragments oder Services}, wird berÃ¼cksichtigt. Mit dieser Kenntnis wird sichergestellt, dass LiveData 
nur App-Komponenten beobachtet, welche sich in einem aktiven Lebenszyklus befinden.
\\ 
\linebreak
Vorteile die die \textit{LiveData}-Komponente mit sich bringt sind unter anderem die Sicherstellung der Ãœbereinstimmung der BenutzeroberflÃ¤che und 
deren Datenstatus, vorbeugend gegenÃ¼ber Speicherverlust, immer auf dem aktuellsten Stand der gespeicherten Informationen und der Wegfall 
der manuellen Lebenszykluskoordination. \cite{livedata.2020}
\subsection*{Room}
\label{sec:Room}
\textit{Room} ist eine Persistenzbibliothek und bietet ein Abstraktionsschicht Ã¼ber der eigentlichen \textit{\acs{SQL}ite}-Datenbank, um einen 
robusten Datenbankzugriff zu managen und das volle Potential von \acs{SQL}ite gleichzeitig zu nutzen. \cite{room.2017}
\\ 
\linebreak
Die robuste \acs{SQL}-Objektzuordnungsbibliothek \textit{Room} kann einen Cache mit den Daten der Applikation auf einem GerÃ¤t erstellen. Dieser 
Cache dient als Wahrheitsquelle der Applikation und ermÃ¶glicht den Benutzern eine konsistente Kopie der Informationen der App anzuzeigen 
und immer zugÃ¤nglich zu machen. \textit{Room} kann in drei Kategorien, bzw. in drei Hauptkomponenten unterteilt werden:
\begin{itemize}
    \item Room database: Dient als Zugriffspunkt fÃ¼r die zugrundeliegende SQLite-Datenbank
    \item DAO (Data Access Object): Beinhaltet Methoden um Datenbankzugriffe zu gewÃ¤hrleisten.
    \item Entity: ReprÃ¤sentiert ein Objekttabelle der Datenbank.
\end{itemize}
Das Diagramm \ref{pic:roomarchitecturediagramm} veranschaulicht die Interaktion, bzw. Funktionsweisen der Hauptkomponenten.
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=15cm,height=7.5cm,keepaspectratio]{2Grundlagen/Bilder/roomArchitecture.png}
    \caption{Room Architektur Diagramm \cite{roomdiagr.2017}}
    \label{pic:roomarchitecturediagramm}
\end{figure} 
\\ 
Allgemein ermÃ¶glicht \textit{Room} einen vereinfachten Zugriff auf die Datenbank, hat einen hohen Grad an \textit{â€testabilityâ€œ}, zur 
Kompilierungszeit wird eine \acs{SQL}-AbfrageÃ¼berprÃ¼fung durchgefÃ¼hrt und interagiert einwandfrei mit anderen Android Architecture 
Components, z.B. LiveData und ViewModel.

\subsection*{ViewModel}
\label{sec:ViewModel}
GrundsÃ¤tzlich dient das \textit{ViewModel} als lebenszyklusbewusste Speicherung und Verwaltung von BenutzeroberflÃ¤chenbezogenen Daten. 
Durch die \textit{ViewModel}-Klasse kÃ¶nnen Informationen KonfigurationsÃ¤nderungen, z.B. Bildschirmdrehungen, die als Ã„nderung zÃ¤hlen, 
Ã¼berstehen. Eine Konfiguration kann sog. Activities verwerfen und neu laden, so kÃ¶nnen nicht persistierte Daten verloren gehen. 
\\ 
\linebreak
Ein \textit{ViewModel} fungiert ebenso als Kommunikationsschnittstelle zwischen den darÃ¼ber- und darunterliegenden Komponenten der 
Architektur (siehe Abschnitt \ref{chap:AAC}). Des Weiteren kann die \textit{ViewModel}-Komponente Informationen zwischen BenutzeroberflÃ¤chen 
und verschiedenen Fragmenten, engl. \textit{â€fragmentsâ€œ} \footnote{Teil einer \ac{UI} in einer Android Activity}, teilen.
\\ 
\linebreak
Die Abbildung \ref{pic:viewModeldiagramm} zeigt eine sich Ã¤ndernde Activity, die zu Beginn und nach Neuerstellung oder Aktualisierung Zugriff 
auf die unverÃ¤nderten Daten hat. 
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=15cm,height=7.5cm,keepaspectratio]{2Grundlagen/Bilder/viewModelDiagram.png}
    \caption{ViewModel Struktur Diagramm \cite{viewmodeldiagr.2020}}
    \label{pic:viewModeldiagramm}
\end{figure} 

\subsection{Sceneform SDK}
Das Sceneform \acs{SDK} ist ein Open-Source Projekt der Google LLC., welches fÃ¼r das Rendern von 3D-Szenen und -Animationen zustÃ¤ndig ist. 
Basierend auf der physikalischen Echtzeit-Rendering-Engine, \textit{physically based renderer} fÃ¼r Android, iOS, macOS, Linux und Windows 
von Google Filament, wurde Sceneform entwickelt. 
\\ 
\linebreak
Bei dem Ansatz des physikalisch basierten Renderns in der Computergrafik wird versucht, alle ModalitÃ¤ten der realen Welt genauer zu modellieren. 
Mit dieser Methode werden virtuelle 3D-Objekte in der \acl{AR} noch realer dargestellt, um einen Echtheitseffekt zu erzielen und simulieren. 
Ein Beispiel ist das simulieren des Lichtflusses, um einen Schatten des Objekts zu erzeugen, damit die Wirksamkeit immer mehr der RealitÃ¤t 
entspricht.
\\ 
\linebreak
Dreidimensionale Modelle werden als \textit{.obj}-Dateien in das Projekt importiert. Diese Dateien beinhalten aufgelistete Punkte, Vektoren 
und Linien, welche das Modell reprÃ¤sentieren. Das \textit{Sceneform \acs{SDK}} verwendet die \textit{.obj}-Datei und konvertiert, bzw. 
rendert diese in eine, \textit{Sceneform binary assets, .sfb}-Datei, um daraus das dreidimensionale Objekt zu generieren, welches anschlieÃŸend 
in \acs{AR}-Anwendungen basierend auf Google ARCore verwendet werden kann. FÃ¼r die finale Renderung und Anzeige ist die 
\textit{ModelRenderable}-Klasse des \acs{SDK}s zustÃ¤ndig. 
%\\ 
%\linebreak
%Ein weiterer wichtiger Aspekt diese Projekts ist der Einsatz einer Datenbank, um generierte Daten speichern zu kÃ¶nnen.
\subsection{SQLite}
\acs{SQL}ite ist eine Open-Source In-Process-Bibliothek, die ein in sich geschlossenes, serverloses, transaktionsfreies \acs{SQL}-Datenbankmodul 
ohne Konfiguration implementiert. \acs{SQL}ite ist eine eingebettete \acs{SQL}-Datenbank-Engine die im Gegensatz zu anderen \acs{SQL}-Datenbanken Ã¼ber 
keinen separaten Serverprozess verfÃ¼gt, d.h. Transaktionen, Lese- und Schreibzugriffe werden direkt auf eine normale Festplattendatei getÃ¤tigt. \cite{sqlite.2018j}
\\
DarÃ¼ber hinaus ist \acs{SQL}ite PlattformÃ¼bergreifend und kann beliebig Datenbank- und Objekttabellen, Indizes und Ansichten erstellen und verwalten. 